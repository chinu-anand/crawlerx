# CrawlerX

<div align="center">
  <h3>A high-performance web crawler with real-time monitoring</h3>
</div>

<div align="center">
  
[![Go Version](https://img.shields.io/github/go-mod/go-version/chinu-anand/crawlerx)](https://github.com/chinu-anand/crawlerx)
  
</div>

## ğŸš€ Overview

CrawlerX is a robust, scalable web crawling system built in Go. It allows you to extract information from websites efficiently with real-time progress monitoring through WebSockets. The system uses a distributed architecture with Redis for job queuing and PostgreSQL for data persistence.

```
ğŸ” Crawl any website
ğŸ“Š Monitor progress in real-time
ğŸ”„ Automatic retry mechanism
ğŸ“± RESTful API for easy integration
ğŸ³ Docker ready for quick deployment
```

## ğŸ—ï¸ Architecture

CrawlerX follows a clean, modular architecture that separates concerns and enables scalability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â”‚  API Layer  â”‚â”€â”€â”€â”€â–¶â”‚  Job Queue  â”‚â”€â”€â”€â”€â–¶â”‚   Workers   â”‚
â”‚             â”‚     â”‚   (Redis)   â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚
       â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚                         â”‚             â”‚
â”‚  WebSocket  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Crawler   â”‚
â”‚    Hub      â”‚                         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚
       â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚                         â”‚             â”‚
â”‚   Clients   â”‚                         â”‚  Database   â”‚
â”‚             â”‚                         â”‚(PostgreSQL) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **High-performance crawling**: Efficiently extracts data from web pages
- **Real-time updates**: WebSocket integration for live progress monitoring
- **Retry mechanism**: Automatic retry for failed requests with configurable delay
- **Distributed architecture**: Redis-based job queue for horizontal scaling
- **Persistent storage**: PostgreSQL database for storing crawl results
- **RESTful API**: Simple API for job submission and monitoring
- **Docker support**: Easy deployment with Docker and docker-compose
- **Export functionality**: Export crawl results in various formats

## ğŸ› ï¸ Tech Stack

- **Backend**: Go (Golang)
- **Web Framework**: Gin
- **Database**: PostgreSQL
- **Queue**: Redis
- **WebSockets**: Gorilla WebSocket
- **HTML Parsing**: GoQuery
- **Configuration**: Viper
- **Containerization**: Docker

## ğŸ“‹ API Endpoints

| Method | Endpoint          | Description                                |
|--------|-------------------|--------------------------------------------|
| GET    | /api/v1/health    | Health check endpoint                      |
| GET    | /api/v1/ws        | WebSocket connection for real-time updates |
| POST   | /api/v1/crawl     | Submit a new crawl job                     |
| GET    | /api/v1/status/:id| Get status of a specific job               |
| GET    | /api/v1/jobs      | List all jobs                              |
| GET    | /api/v1/jobs/:id  | Get detailed information about a job       |
| GET    | /api/v1/jobs/export/:id | Export job results                   |
| GET    | /api/v1/jobs/summary    | Get summary of all jobs              |

## ğŸš€ Getting Started

### Prerequisites

- Go 1.18 or higher
- Docker and docker-compose (for containerized deployment)
- PostgreSQL (if running locally)
- Redis (if running locally)

### Local Development

1. Clone the repository:
   ```bash
   git clone https://github.com/chinu-anand/crawlerx.git
   cd crawlerx
   ```

2. Install dependencies:
   ```bash
   go mod download
   ```

3. Set up environment variables (create a `.env` file):
   ```
   PORT=8080
   DATABASE_URL=postgres://postgres:postgres@localhost:5432/crawlerx?sslmode=disable
   REDIS_URL=redis://localhost:6379/0
   ```

4. Run the application:
   ```bash
   go run cmd/server/main.go
   ```

### Docker Deployment

1. Build and start the containers:
   ```bash
   docker-compose up -d
   ```

2. The application will be available at `http://localhost:8080`

## ğŸ“ Usage Examples

### Submit a Crawl Job

```bash
curl -X POST http://localhost:8080/api/v1/crawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

Response:
```json
{
  "message": "Crawl request received",
  "id": "550e8400-e29b-41d4-a716-446655440000"
}
```

### Get Job Status

```bash
curl http://localhost:8080/api/v1/status/550e8400-e29b-41d4-a716-446655440000
```

Response:
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "url": "https://example.com",
  "status": "done",
  "title": "Example Domain",
  "description": "Example website for demonstration purposes",
  "links": ["https://www.iana.org/domains/example"]
}
```

## ğŸ”„ WebSocket Integration

Connect to the WebSocket endpoint to receive real-time updates:

```javascript
const socket = new WebSocket('ws://localhost:8080/api/v1/ws');

socket.onmessage = function(event) {
  const data = JSON.parse(event.data);
  console.log('Job update:', data);
};
```

## ğŸ§© Project Structure

```
crawlerx/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go         # Application entry point
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.go           # Configuration management
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ api/                # API handlers and routes
â”‚   â”œâ”€â”€ crawler/            # Web crawling implementation
â”‚   â”œâ”€â”€ models/             # Database models
â”‚   â”œâ”€â”€ queue/              # Job queue implementation
â”‚   â”œâ”€â”€ worker/             # Worker processing logic
â”‚   â””â”€â”€ ws/                 # WebSocket implementation
â”œâ”€â”€ pkg/                    # Shared packages
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ docker-compose.yaml     # Docker Compose configuration
â”œâ”€â”€ go.mod                  # Go module definition
â””â”€â”€ README.md               # Project documentation
```

## ğŸ“Š Performance

CrawlerX is designed for high performance and scalability:

- **Concurrent processing**: Multiple workers can process jobs simultaneously
- **Efficient HTML parsing**: Using GoQuery for optimized DOM traversal
- **Minimal memory footprint**: Careful resource management
- **Horizontal scaling**: Add more worker instances as needed